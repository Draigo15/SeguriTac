import { addDoc, collection, serverTimestamp } from 'firebase/firestore';
import { db } from './firebase';

/**
 * Configuraci√≥n del chatbot inteligente
 */
interface ChatbotConfig {
  useLocalModel: boolean;
  ollamaUrl: string;
  modelName: string;
  maxTokens: number;
  temperature: number;
  fallbackToPreset: boolean;
  rateLimitPerHour: number;
}

/**
 * Cache para respuestas del chatbot
 */
class ResponseCache {
  private cache = new Map<string, { response: string; timestamp: number }>();
  private readonly CACHE_DURATION = 3600000; // 1 hora

  get(message: string): string | null {
    const key = this.normalizeMessage(message);
    const cached = this.cache.get(key);
    
    if (cached && Date.now() - cached.timestamp < this.CACHE_DURATION) {
      return cached.response;
    }
    
    return null;
  }

  set(message: string, response: string): void {
    const key = this.normalizeMessage(message);
    this.cache.set(key, { response, timestamp: Date.now() });
  }

  getSize(): number {
    return this.cache.size;
  }

  clear(): void {
    this.cache.clear();
  }

  private normalizeMessage(message: string): string {
    return message.toLowerCase().trim();
  }
}

/**
 * Rate limiter para controlar uso de IA
 */
class RateLimiter {
  private requests = new Map<string, number[]>();

  canMakeRequest(userId: string, limit: number = 10): boolean {
    const now = Date.now();
    const userRequests = this.requests.get(userId) || [];
    
    // Filtrar requests de la √∫ltima hora
    const recentRequests = userRequests.filter(time => now - time < 3600000);
    
    if (recentRequests.length >= limit) {
      return false;
    }
    
    recentRequests.push(now);
    this.requests.set(userId, recentRequests);
    return true;
  }

  getRemainingRequests(userId: string, limit: number = 10): number {
    const now = Date.now();
    const userRequests = this.requests.get(userId) || [];
    const recentRequests = userRequests.filter(time => now - time < 3600000);
    return Math.max(0, limit - recentRequests.length);
  }
}

/**
 * Servicio de chatbot inteligente para seguridad ciudadana
 * Combina IA local (Ollama) con respuestas predefinidas como fallback
 */
export const chatbotService = {
  // Configuraci√≥n del chatbot
  config: {
    useLocalModel: true,
    ollamaUrl: 'http://localhost:11434',
    modelName: 'llama3.1:8b',
    maxTokens: 150,
    temperature: 0.7,
    fallbackToPreset: true,
    rateLimitPerHour: 20
  } as ChatbotConfig,

  // Instancias de cache y rate limiter
  cache: new ResponseCache(),
  rateLimiter: new RateLimiter(),

  /**
   * Prompt especializado para seguridad ciudadana
   */
  securityPrompt: `Eres un asistente virtual especializado en seguridad ciudadana en espa√±ol.

Instrucciones:
- Responde SIEMPRE en espa√±ol
- M√°ximo 100 palabras por respuesta
- S√© emp√°tico y profesional
- Si detectas emergencia, recomienda llamar al 911 INMEDIATAMENTE
- Para reportes, gu√≠a al usuario a usar la aplicaci√≥n
- Proporciona consejos de seguridad cuando sea apropiado
- Si no sabes algo espec√≠fico, deriva a las autoridades

Usuario: {message}

Respuesta:`,

  /**
   * Respuestas predefinidas del chatbot basadas en palabras clave
   */
  responses: {
    // Saludos
    greeting: [
      '¬°Hola! Soy el asistente virtual de Seguridad Ciudadana. Estoy aqu√≠ para ayudarte mientras esperas respuesta de las autoridades.',
      'Hola, soy tu asistente virtual. ¬øEn qu√© puedo ayudarte mientras esperamos respuesta oficial?',
    ],
    
    // Informaci√≥n sobre el proceso
    process: [
      'Tu reporte ha sido enviado correctamente a las autoridades competentes. El tiempo de respuesta promedio es de 2-4 horas.',
      'Las autoridades revisan los reportes por orden de prioridad. Los casos de emergencia se atienden inmediatamente.',
      'Puedes seguir el estado de tu reporte desde la secci√≥n "Mis Reportes" en el men√∫ principal.',
    ],
    
    // Emergencias
    emergency: [
      'üö® Si es una EMERGENCIA INMEDIATA, usa el bot√≥n de emergencia en la pantalla principal o llama al 911.',
      '‚ö†Ô∏è Para emergencias que requieren atenci√≥n inmediata, contacta directamente al 911 o servicios de emergencia.',
      'Si tu situaci√≥n es urgente, no esperes respuesta del chat. Contacta inmediatamente a los servicios de emergencia.',
    ],
    
    // Informaci√≥n sobre tipos de reportes
    reportTypes: [
      'Los reportes se clasifican en: Robo, Violencia, Accidente, Emergencia, Vandalismo, Drogas, Ruido, Tr√°fico y Otros.',
      'Cada tipo de reporte tiene diferentes tiempos de respuesta seg√∫n su prioridad y gravedad.',
    ],
    
    // Consejos de seguridad
    safety: [
      'üõ°Ô∏è Consejos de seguridad: Mantente en lugares bien iluminados, evita mostrar objetos de valor, y conf√≠a en tu instinto.',
      'üö∂‚Äç‚ôÄÔ∏è Para tu seguridad: Camina con confianza, mantente alerta a tu entorno, y comparte tu ubicaci√≥n con familiares.',
      'üì± Mant√©n tu tel√©fono cargado, ten n√∫meros de emergencia a mano, y conoce las rutas seguras en tu zona.',
    ],
    
    // Informaci√≥n sobre evidencia
    evidence: [
      'üì∏ La evidencia fotogr√°fica ayuda mucho a las autoridades. Aseg√∫rate de que las im√°genes sean claras y relevantes.',
      'üìù Proporciona todos los detalles posibles: hora exacta, ubicaci√≥n precisa, descripci√≥n de personas involucradas.',
      'Si tienes testigos, incluye sus datos de contacto en la descripci√≥n del reporte.',
    ],
    
    // Seguimiento
    followUp: [
      'Recibir√°s notificaciones cuando las autoridades respondan a tu reporte.',
      'Puedes agregar informaci√≥n adicional respondiendo en este chat cuando las autoridades se conecten.',
      'El estado de tu reporte se actualiza autom√°ticamente: Pendiente ‚Üí En Proceso ‚Üí Resuelto.',
    ],
    
    // Ayuda general
    help: [
      '‚ùì Puedo ayudarte con: informaci√≥n sobre el proceso, consejos de seguridad, tipos de reportes, y seguimiento.',
      'ü§ñ Soy un asistente autom√°tico. Para casos espec√≠ficos, espera la respuesta de las autoridades.',
      'Si necesitas ayuda con la app, ve a Configuraci√≥n ‚Üí Ayuda, o contacta soporte t√©cnico.',
    ],
    
    // Respuesta por defecto
    default: [
      'Entiendo tu preocupaci√≥n. Las autoridades revisar√°n tu reporte y te responder√°n pronto.',
      'Gracias por tu mensaje. Mientras esperamos respuesta oficial, ¬øhay algo m√°s en lo que pueda ayudarte?',
      'Tu reporte es importante para nosotros. Las autoridades competentes lo est√°n revisando.',
    ],
  },

  /**
   * Palabras clave para identificar el tipo de consulta
   */
  keywords: {
    greeting: ['hola', 'buenos', 'buenas', 'saludos', 'hey'],
    process: ['proceso', 'tiempo', 'demora', 'cuanto', 'cuando', 'respuesta', 'estado'],
    emergency: ['emergencia', 'urgente', 'ayuda', 'rapido', 'inmediato', '911'],
    reportTypes: ['tipo', 'categoria', 'clasificacion', 'reportes'],
    safety: ['seguridad', 'consejos', 'proteccion', 'cuidado', 'prevenir'],
    evidence: ['evidencia', 'foto', 'imagen', 'prueba', 'testigo'],
    followUp: ['seguimiento', 'notificacion', 'actualizar', 'estado'],
    help: ['ayuda', 'como', 'que puedes', 'funciones', 'soporte'],
  },

  /**
   * Procesa mensaje con Ollama (IA local)
   */
  async processWithOllama(message: string): Promise<string> {
    try {
      const prompt = this.securityPrompt.replace('{message}', message);
      
      const response = await fetch(`${this.config.ollamaUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.config.modelName,
          prompt: prompt,
          stream: false,
          options: {
            num_predict: this.config.maxTokens,
            temperature: this.config.temperature,
            stop: ['Usuario:', 'Respuesta:']
          }
        }),
      });

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.status}`);
      }

      const data = await response.json();
      return data.response?.trim() || this.getPresetResponse(message);
    } catch (error) {
      console.log('Ollama no disponible, usando respuestas predefinidas:', error);
      throw error;
    }
  },

  /**
   * Procesa mensaje del usuario con IA inteligente y fallbacks
   */
  async processMessage(message: string, userId: string = 'anonymous'): Promise<string> {
    try {
      // 1. Verificar cache primero
      const cachedResponse = this.cache.get(message);
      if (cachedResponse) {
        return cachedResponse;
      }

      // 2. Verificar rate limit
      if (!this.rateLimiter.canMakeRequest(userId, this.config.rateLimitPerHour)) {
        const remaining = this.rateLimiter.getRemainingRequests(userId, this.config.rateLimitPerHour);
        return `Has alcanzado el l√≠mite de consultas por hora. Podr√°s hacer ${remaining} consultas m√°s en la pr√≥xima hora. Mientras tanto, puedo ayudarte con respuestas b√°sicas.`;
      }

      let response: string;

      // 3. Intentar con IA local (Ollama)
      if (this.config.useLocalModel) {
        try {
          response = await this.processWithOllama(message);
          this.cache.set(message, response);
          return response;
        } catch (error) {
          console.log('ü§ñ Ollama no disponible, usando respuestas predefinidas');
        }
      }

      // 4. Fallback a respuestas predefinidas
      response = this.getPresetResponse(message);
      this.cache.set(message, response);
      return response;

    } catch (error) {
      console.error('Error procesando mensaje:', error);
      return 'Disculpa, estoy teniendo problemas t√©cnicos. Por favor, contacta directamente a las autoridades si es urgente.';
    }
  },

  /**
   * Obtiene respuesta predefinida basada en palabras clave
   */
  getPresetResponse(message: string): string {
    const lowerMessage = message.toLowerCase();
    
    for (const [category, keywords] of Object.entries(this.keywords)) {
      if (keywords.some(keyword => lowerMessage.includes(keyword))) {
        const responses = this.responses[category as keyof typeof this.responses];
        return responses[Math.floor(Math.random() * responses.length)];
      }
    }
    
    return this.responses.default[Math.floor(Math.random() * this.responses.default.length)];
  },

  /**
   * Analiza el mensaje del usuario y determina el tipo de respuesta (m√©todo legacy)
   */
  analyzeMessage(message: string): string {
    return this.getPresetResponse(message);
  },

  /**
   * Env√≠a una respuesta inteligente del bot al chat
   */
  async sendBotResponse(reportId: string, userMessage: string, userId: string = 'anonymous'): Promise<void> {
    try {
      // Usar el nuevo sistema inteligente
      const botResponse = await this.processMessage(userMessage, userId);
      
      // Agregar un peque√±o delay para simular "typing"
      await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 2000));
      
      await addDoc(collection(db, 'report_chats', reportId, 'messages'), {
        text: botResponse,
        sender: 'chatbot',
        uid: 'chatbot-assistant',
        timestamp: serverTimestamp(),
        isBot: true,
        type: 'intelligent_bot_response',
        metadata: {
          usedAI: this.config.useLocalModel,
          cached: this.cache.get(userMessage) !== null,
          timestamp: new Date().toISOString()
        }
      });
      
      console.log('ü§ñ Respuesta inteligente del bot enviada:', botResponse.substring(0, 50) + '...');
    } catch (error) {
      console.error('‚ùå Error al enviar respuesta del chatbot:', error);
      
      // Fallback de emergencia
      try {
        const fallbackResponse = this.getPresetResponse(userMessage);
        await addDoc(collection(db, 'report_chats', reportId, 'messages'), {
          text: fallbackResponse,
          sender: 'chatbot',
          uid: 'chatbot-assistant',
          timestamp: serverTimestamp(),
          isBot: true,
          type: 'fallback_response'
        });
      } catch (fallbackError) {
        console.error('‚ùå Error en fallback:', fallbackError);
      }
    }
  },

  /**
   * Env√≠a mensaje de bienvenida cuando se abre el chat por primera vez
   */
  async sendWelcomeMessage(reportId: string): Promise<void> {
    try {
      const welcomeMessage = this.responses.greeting[0] + ' \n\n' +
        'Mientras esperamos respuesta de las autoridades, puedo ayudarte con:\n' +
        '‚Ä¢ Informaci√≥n sobre el proceso\n' +
        '‚Ä¢ Consejos de seguridad\n' +
        '‚Ä¢ Seguimiento de tu reporte\n' +
        '‚Ä¢ Respuestas a preguntas frecuentes\n\n' +
        '¬øEn qu√© puedo ayudarte?';
      
      await addDoc(collection(db, 'report_chats', reportId, 'messages'), {
        text: welcomeMessage,
        sender: 'chatbot',
        uid: 'chatbot-assistant',
        timestamp: serverTimestamp(),
        isBot: true,
      });
      
      console.log('ü§ñ Mensaje de bienvenida del chatbot enviado');
    } catch (error) {
      console.error('‚ùå Error al enviar mensaje de bienvenida del chatbot:', error);
    }
  },

  /**
   * Verifica si debe responder autom√°ticamente
   */
  shouldRespond(messages: any[], userRole: string): boolean {
    // Solo responder a ciudadanos, no a autoridades
    if (userRole === 'autoridad') return false;
    
    // No responder si ya hay una autoridad activa en el chat
    const hasAuthorityMessage = messages.some(msg => 
      msg.sender === 'autoridad' && 
      msg.timestamp && 
      (Date.now() - msg.timestamp.toDate().getTime()) < 3600000 // 1 hora
    );
    
    if (hasAuthorityMessage) return false;
    
    // Responder si es el primer mensaje del usuario o si han pasado m√°s de 5 minutos
    const userMessages = messages.filter(msg => msg.sender === 'ciudadano');
    if (userMessages.length === 1) return true;
    
    const lastBotMessage = messages
      .filter(msg => msg.sender === 'chatbot')
      .sort((a, b) => b.timestamp?.toDate().getTime() - a.timestamp?.toDate().getTime())[0];
    
    if (!lastBotMessage) return true;
    
    const timeSinceLastBot = Date.now() - lastBotMessage.timestamp.toDate().getTime();
    return timeSinceLastBot > 300000; // 5 minutos
  },

  /**
   * Verifica si Ollama est√° disponible
   */
  async checkOllamaStatus(): Promise<{
    available: boolean;
    model?: string;
    url?: string;
    error?: string;
  }> {
    try {
      const response = await fetch(`${this.config.ollamaUrl}/api/tags`, {
        method: 'GET',
        signal: AbortSignal.timeout(5000) // 5 segundos timeout
      });
      
      if (response.ok) {
        return {
          available: true,
          model: this.config.modelName,
          url: this.config.ollamaUrl
        };
      } else {
        return {
          available: false,
          error: `HTTP ${response.status}: ${response.statusText}`
        };
      }
    } catch (error) {
      console.log('Ollama no est√° disponible:', error);
      return {
        available: false,
        error: error instanceof Error ? error.message : 'Error desconocido'
      };
    }
  },

  /**
   * Configura el chatbot (permite cambiar configuraci√≥n en tiempo real)
   */
  updateConfig(newConfig: Partial<ChatbotConfig>): void {
    this.config = { ...this.config, ...newConfig };
    console.log('Configuraci√≥n del chatbot actualizada:', this.config);
  },

  /**
   * Obtiene estad√≠sticas del chatbot
   */
  getStats(userId: string = 'anonymous') {
    return {
      cacheSize: this.cache.getSize(),
      remainingRequests: this.rateLimiter.getRemainingRequests(userId, this.config.rateLimitPerHour),
      ollamaEnabled: this.config.useLocalModel,
      modelName: this.config.modelName,
      rateLimitPerHour: this.config.rateLimitPerHour
    };
  },

  /**
   * Limpia el cache del chatbot
   */
  clearCache(): void {
    this.cache.clear();
    console.log('Cache del chatbot limpiado');
  },

  /**
   * Mensaje de bienvenida inteligente
   */
  async sendIntelligentWelcomeMessage(reportId: string, userId: string = 'anonymous'): Promise<void> {
    try {
      const welcomeMessage = `Hola, soy tu asistente virtual de seguridad ciudadana con IA. Tu reporte ha sido recibido correctamente. ¬øEn qu√© puedo ayudarte mientras esperamos respuesta de las autoridades?`;
      
      await addDoc(collection(db, 'chats', reportId, 'messages'), {
        text: welcomeMessage,
        senderId: 'system',
        senderName: 'ü§ñ Asistente Virtual IA',
        timestamp: serverTimestamp(),
        isBot: true,
        type: 'welcome_message',
        metadata: {
          aiEnabled: this.config.useLocalModel,
          timestamp: new Date().toISOString()
        }
      });
      
      console.log('Mensaje de bienvenida inteligente enviado');
    } catch (error) {
      console.error('Error enviando mensaje de bienvenida:', error);
      // Fallback al mensaje de bienvenida original
      await this.sendWelcomeMessage(reportId);
    }
  },
};